## Crawling

### Summary
Crawling (spidering) automatically follows links from a seed URL to discover pages and extract recon data like endpoints, emails, comments, and exposed files.

------------------------------------------------------------

## EXERCISE: Crawl Target with Scrapy + ReconSpider

### Task 1: Install Scrapy

1. Run: `pip3 install scrapy`
   - Target-agnostic: `pip3 install scrapy`
   - Example result:
     - `Successfully installed scrapy-<VERSION> ...`

------------------------------------------------------------

### Task 2: Download and extract ReconSpider

1. Run: `wget -O ReconSpider.zip https://academy.hackthebox.com/storage/modules/144/ReconSpider.v1.2.zip`
   - Target-agnostic: `wget -O ReconSpider.zip <RECONSPIDER_ZIP_URL>`
   - Example result:
     - `Saving to: 'ReconSpider.zip'`

2. Run: `unzip ReconSpider.zip`
   - Target-agnostic: `unzip ReconSpider.zip`
   - Example result:
     - `inflating: ReconSpider.py`
     - `inflating: ...`

------------------------------------------------------------

### Task 3: Run the spider against a target

1. Run: `python3 ReconSpider.py http://inlanefreight.com`
   - Target-agnostic: `python3 ReconSpider.py <TARGET_URL>`
   - Example result:
     - `results.json created`
     - `(output may be minimal; primary results are written to file)`

------------------------------------------------------------

## EXERCISE: Review Output (results.json)

### Task 1: Understand extracted data categories

1. Output file: `results.json`
   - Target-agnostic: `results.json`
   - Example result structure:
     - `"emails": ["user@<DOMAIN>", "..."]`
     - `"links": ["http(s)://<DOMAIN>/...", "..."]`
     - `"external_files": ["http(s)://<DOMAIN>/...pdf", "..."]`
     - `"js_files": ["http(s)://<DOMAIN>/...js", "..."]`
     - `"form_fields": []`
     - `"images": ["http(s)://<DOMAIN>/...png", "..."]`
     - `"videos": []`
     - `"audio": []`
     - `"comments": ["<!-- ... -->", "..."]`
